from ingest_text_files import get_retriever, ingest_from_docs, ingest_from_image
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from uuid import uuid4
import os, io, base64, json
from PIL import Image
import matplotlib.pyplot as plt
from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="VBO DE Bootcamp RAG Assistant")

UPLOAD_DIR = "/tmp/uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

print("ğŸ“š BaÅŸlangÄ±Ã§ kontrol ediliyor...")
retriever = None

try:
    pdf_files = [f for f in os.listdir(UPLOAD_DIR) if f.endswith('.pdf')]
    if pdf_files:
        print(f"ğŸ“„ {len(pdf_files)} PDF dosyasÄ± bulundu, yÃ¼kleniyor...")
        success = ingest_from_docs(UPLOAD_DIR)
        if success:
            retriever = get_retriever()
            print("âœ… Retriever hazÄ±r")
    else:
        print("âš ï¸  BaÅŸlangÄ±Ã§ta PDF bulunamadÄ±.")
except Exception as e:
    print(f"âš ï¸  BaÅŸlangÄ±Ã§ yÃ¼klemesi atlandÄ±: {e}")

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    raise ValueError("âŒ GOOGLE_API_KEY .env'de bulunamadÄ±!")

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.7,
    google_api_key=api_key
)

store = {}
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", "Given chat history and latest question, reformulate if needed"),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

qa_prompt_template = ChatPromptTemplate.from_messages([
    ("system", """Sen matematik ders notu asistanÄ±sÄ±n. 
    AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu yanÄ±tla.
    
    Ã–NEMLÄ°:
    - Kapak, Ã¶nsÃ¶z, fotoÄŸraf gibi kÄ±sÄ±mlarÄ± yoksay
    - Ã–nce baÄŸlamda cevabÄ± ara
    - BaÄŸlamda yoksa, kendi bilgini kullan (LLM olarak)
    - Matematiksel sorularÄ± adÄ±m adÄ±m Ã§Ã¶z
    - Gerekirse diyagram veya grafik oluÅŸtur
    ğŸ“Š GRAFÄ°K/ÅEKÄ°L Ä°STENMESÄ°:
    KullanÄ±cÄ± grafik, diyagram, ÅŸekil, Ã§izim isterse:
    1. CevabÄ± aÃ§Ä±kla
    2. Sonra ÅU FORMATTA grafiksel gÃ¶sterim ver:
    
    GRAPH: [x_deÄŸerleri], [y_deÄŸerleri]
    
    Ã–RNEKLER:
    - Parabol: GRAPH: [-2,-1,0,1,2], [4,1,0,1,4]
    - DoÄŸru: GRAPH: [0,1,2,3], [0,2,4,6]
    - Trigonometrik: GRAPH: [0,1.57,3.14,4.71,6.28], [0,1,0,-1,0]
    - SÃ¼tun grafik: GRAPH: [0,1,2,3], [10,20,15,25]
    - Herhangi veri: GRAPH: [x1,x2,x3,...], [y1,y2,y3,...]
    
    BaÄŸlam:
    {context}
    """),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

history_aware_retriever = None
question_answer_chain = None
rag_chain = None
qa_chain = None

def initialize_chains():
    global history_aware_retriever, question_answer_chain, rag_chain, qa_chain
    if not retriever:
        print("âš ï¸  Retriever yok")
        return False
    try:
        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_q_prompt
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt_template)
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        qa_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        print("âœ… Chain'ler baÅŸlatÄ±ldÄ±")
        return True
    except Exception as e:
        print(f"âŒ Chain baÅŸlatma hatasÄ±: {e}")
        return False

if retriever:
    initialize_chains()

class Message(BaseModel):
    name: str

@app.get("/")
def root():
    return {
        "message": "VBO LLM Bootcamp RAG Assistant",
        "status": "ready" if retriever else "waiting_for_documents",
        "version": "3.0"
    }

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "retriever_ready": retriever is not None,
        "active_sessions": len(store)
    }

# Streaming mesaj endpoint
@app.post("/message")
def send_request(message: Message):
    print(f"ğŸ’¬ Soru alÄ±ndÄ±: {message.name}")
    
    if not retriever:
        raise HTTPException(
            status_code=503,
            detail="HenÃ¼z dokÃ¼man yÃ¼klenmedi. PDF yÃ¼kleyin."
        )
    
    if not qa_chain:
        raise HTTPException(
            status_code=503,
            detail="Chain baÅŸlatÄ±lmadÄ±. PDF yÃ¼kleyin."
        )
    
    try:
        # RAG chain ile yanÄ±t al
        session_id = "default_session"  # Her kullanÄ±cÄ± iÃ§in farklÄ± olabilir
        
        print(f"ğŸ” RAG chain Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        result = qa_chain.invoke(
            {"input": message.name},
            config={"configurable": {"session_id": session_id}}
        )
        
        # âœ… DOÄRU: result bir dict, "answer" anahtarÄ±ndan yanÄ±tÄ± al
        answer = result.get("answer", "YanÄ±t bulunamadÄ±.")
        
        print(f"âœ… YanÄ±t oluÅŸturuldu: {len(answer)} karakter")
        
        # Grafik kontrolÃ¼ (opsiyonel - eÄŸer LLM JSON dÃ¶ndÃ¼rÃ¼rse)
        graph_image_base64 = None
        try:
            # EÄŸer yanÄ±t JSON formatÄ±nda grafik verisi iÃ§eriyorsa
            data = json.loads(answer)
            if data.get("type") == "graph":
                y = data.get("data", [])
                plt.figure()
                plt.plot(y)
                buf = io.BytesIO()
                plt.savefig(buf, format="png")
                buf.seek(0)
                graph_image_base64 = base64.b64encode(buf.read()).decode("utf-8")
                plt.close()
        except:
            # JSON deÄŸilse, GRAPH: formatÄ±nÄ± kontrol et
            if "GRAPH:" in answer:
                try:
                    import re, ast
                    graph_line = re.search(r'GRAPH:\s*\[.*?\],\s*\[.*?\]', answer)
                    if graph_line:
                        coords = graph_line.group().replace("GRAPH:", "").strip()
                        x_vals, y_vals = ast.literal_eval(f"[{coords}]")
                        
                        plt.figure(figsize=(8, 6))
                        plt.plot(x_vals, y_vals, marker='o')
                        plt.grid(True)
                        plt.title("Grafik")
                        
                        buf = io.BytesIO()
                        plt.savefig(buf, format="png", bbox_inches='tight')
                        buf.seek(0)
                        graph_image_base64 = base64.b64encode(buf.read()).decode("utf-8")
                        plt.close()
                        print("ğŸ“Š Grafik oluÅŸturuldu")
                        
                        # GRAPH satÄ±rÄ±nÄ± temizle
                        answer = re.sub(r'GRAPH:\s*\[.*?\],\s*\[.*?\]', '', answer).strip()
                except Exception as graph_error:
                    print(f"âš ï¸ Grafik oluÅŸturulamadÄ±: {graph_error}")

        return {"text": answer, "graph_image": graph_image_base64}

    except Exception as e:
        print(f"âŒ Sohbet hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    if not file.filename.endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Sadece PDF kabul edilir")
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    content = await file.read()
    with open(file_path, "wb") as f:
        f.write(content)
    success = ingest_from_docs(UPLOAD_DIR)
    if not success:
        raise HTTPException(status_code=500, detail="PDF iÅŸlenemedi")
    global retriever
    retriever = get_retriever()
    initialize_chains()
    return {"status":"success","filename":file.filename,"size_bytes":len(content),
            "message":f"{file.filename} baÅŸarÄ±yla yÃ¼klendi ve iÅŸlendi"}

@app.post("/upload-image")
async def analyze_image(file: UploadFile = File(...)):
    print(f"ğŸ“¸ GÃ¶rsel alÄ±ndÄ±: {file.filename}")
    
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="Sadece gÃ¶rsel yÃ¼kleyin")
    
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    content = await file.read()
    with open(file_path, "wb") as f:
        f.write(content)
    
    success = ingest_from_image(file_path)
    if not success:
        raise HTTPException(status_code=500, detail="GÃ¶rsel iÅŸlenemedi")
    
    try:
        with open(file_path, "rb") as img_file:
            image_data = base64.b64encode(img_file.read()).decode("utf-8")
        
        from langchain_core.messages import HumanMessage
        
        # GÃ¶rsel analizi
        analysis_message = HumanMessage(
            content=[
                {
                    "type": "text", 
                    "text": """Bu gÃ¶rseli detaylÄ± analiz et. 
                    EÄŸer bir matematik sorusu varsa adÄ±m adÄ±m Ã§Ã¶z.
                    EÄŸer bir grafik Ã§izilmesi gerekiyorsa, son satÄ±rda ÅŸu formatta belirt:
                    GRAPH: [x_deÄŸerleri], [y_deÄŸerleri]
                    Ã–rnek: GRAPH: [0,1,2,3,4], [0,1,4,9,16]
                    """
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                }
            ]
        )
        
        response = llm.invoke([analysis_message])
        analysis_text = response.content
        
        print(f"âœ… Analiz: {analysis_text[:200]}...")
        
        # Grafik verisi var mÄ± kontrol et
        graph_image_base64 = None
        if "GRAPH:" in analysis_text:
            try:
                import re
                import ast
                
                # GRAPH: satÄ±rÄ±nÄ± bul
                graph_line = re.search(r'GRAPH:\s*\[.*?\],\s*\[.*?\]', analysis_text)
                if graph_line:
                    # Verileri parse et
                    coords = graph_line.group().replace("GRAPH:", "").strip()
                    x_vals, y_vals = ast.literal_eval(f"[{coords}]")
                    
                    # Grafik Ã§iz
                    plt.figure(figsize=(8, 6))
                    plt.plot(x_vals, y_vals, marker='o')
                    plt.grid(True)
                    plt.title("Grafik")
                    
                    buf = io.BytesIO()
                    plt.savefig(buf, format="png", bbox_inches='tight')
                    buf.seek(0)
                    graph_image_base64 = base64.b64encode(buf.read()).decode("utf-8")
                    plt.close()
                    
                    print("ğŸ“Š Grafik oluÅŸturuldu")
                    
                    # GRAPH satÄ±rÄ±nÄ± temizle
                    analysis_text = re.sub(r'GRAPH:\s*\[.*?\],\s*\[.*?\]', '', analysis_text).strip()
            except Exception as graph_error:
                print(f"âš ï¸ Grafik oluÅŸturulamadÄ±: {graph_error}")
        
        return {
            "status": "success",
            "message": "GÃ¶rsel baÅŸarÄ±yla analiz edildi",
            "analysis": analysis_text,
            "graph_image": graph_image_base64,
            "filename": file.filename
        }
        
    except Exception as e:
        print(f"âŒ Hata: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Analiz hatasÄ±: {str(e)}")