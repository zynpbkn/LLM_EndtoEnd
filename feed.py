from ingest_text_files import get_retriever, ingest_from_docs  # âœ… get_retriever
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from uuid import uuid4
import os

from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="VBO DE Bootcamp RAG Assistant")

# Upload dizini
UPLOAD_DIR = "/tmp/uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# âœ… Ä°lk baÅŸta mevcut PDF'leri yÃ¼kle (gÃ¼venli ÅŸekilde)
print("ğŸ“š BaÅŸlangÄ±Ã§ kontrol ediliyor...")
retriever = None

try:
    # Dizinde PDF var mÄ± kontrol et
    pdf_files = [f for f in os.listdir(UPLOAD_DIR) if f.endswith('.pdf')]
    
    if pdf_files:
        print(f"ğŸ“„ {len(pdf_files)} PDF dosyasÄ± bulundu, yÃ¼kleniyor...")
        success = ingest_from_docs(UPLOAD_DIR)
        if success:
            retriever = get_retriever()
            print("âœ… Retriever hazÄ±r")
    else:
        print("âš ï¸  BaÅŸlangÄ±Ã§ta PDF bulunamadÄ±.")
except Exception as e:
    print(f"âš ï¸  BaÅŸlangÄ±Ã§ yÃ¼klemesi atlandÄ±: {e}")

# âœ… Model - Gemini 1.5 kullanÄ±n (2.0 henÃ¼z stable olmayabilir)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",  # veya "gemini-1.5-pro"
    temperature=0.7
)

# Session store for chat history
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Contextualize question prompt
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# Answer question prompt template (global olarak tanÄ±mla)
qa_prompt_template = ChatPromptTemplate.from_messages([
    ("system", "Sen akÄ±llÄ± ders notu asistanÄ±sÄ±n. Sana sorulan sorularÄ± tanÄ± ve aÃ§Ä±kla, Ã¶zetle, konsepti gÃ¶rselleÅŸtir, Ã¶rnek sorular Ã¼ret, diyagram oluÅŸtur.\n\nContext: {context}"),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# âœ… Chain'leri baÅŸlat (eÄŸer retriever varsa)
history_aware_retriever = None
question_answer_chain = None
rag_chain = None
qa_chain = None

def initialize_chains():
    """Chain'leri baÅŸlat veya yeniden baÅŸlat"""
    global history_aware_retriever, question_answer_chain, rag_chain, qa_chain
    
    if not retriever:
        print("âš ï¸  Retriever yok, chain'ler baÅŸlatÄ±lamadÄ±")
        return False
    
    try:
        # Create history-aware retriever
        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_q_prompt
        )

        # Create question-answer chain
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt_template)

        # Create retrieval chain
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        # Add message history
        qa_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        
        print("âœ… Chain'ler baÅŸlatÄ±ldÄ±")
        return True
    except Exception as e:
        print(f"âŒ Chain baÅŸlatma hatasÄ±: {e}")
        return False

# Ä°lk chain baÅŸlatma
if retriever:
    initialize_chains()

# Pydantic model
class Message(BaseModel):
    name: str

# Root endpoint
@app.get("/")
def root():
    return {
        "message": "VBO DE Bootcamp RAG Assistant",
        "status": "ready" if retriever else "waiting_for_documents",
        "version": "1.0"
    }

# Health check endpoint
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "retriever_ready": retriever is not None,
        "active_sessions": len(store)
    }

# Message endpoint with streaming
@app.post("/message")
def send_request(message: Message):
    # âœ… Retriever kontrolÃ¼
    if not retriever:
        raise HTTPException(
            status_code=503,
            detail="HenÃ¼z dokÃ¼man yÃ¼klenmedi. LÃ¼tfen Ã¶nce bir PDF yÃ¼kleyin."
        )
    
    try:
        def generate_response():
            # Get relevant documents first
            docs = retriever.get_relevant_documents(message.name)
            
            if not docs:
                yield "âš ï¸ Ä°lgili dokÃ¼man bulunamadÄ±. LÃ¼tfen daha spesifik bir soru sorun."
                return
            
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # Create a simple prompt for streaming
            prompt = f"""Sen akÄ±llÄ± ders notu asistanÄ±sÄ±n. Sana sorulan sorularÄ± tanÄ± ve aÃ§Ä±kla, Ã¶zetle, konsepti gÃ¶rselleÅŸtir, Ã¶rnek sorular Ã¼ret, diyagram oluÅŸtur.

Context: {context}

Soru: {message.name}

Cevap:"""
            
            # Stream directly from LLM
            for chunk in llm.stream(prompt):
                if chunk.content:
                    yield chunk.content
        
        return StreamingResponse(generate_response(), media_type="text/plain")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")

# PDF UPLOAD ENDPOINT
@app.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    try:
        # Dosya tipini kontrol et
        if not file.filename.endswith(".pdf"):
            raise HTTPException(
                status_code=400, 
                detail="Sadece PDF dosyalarÄ± kabul edilir (.pdf)"
            )

        # DosyayÄ± kaydet
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)

        print(f"ğŸ“„ {file.filename} kaydedildi, iÅŸleniyor...")
        
        # Qdrant'a yÃ¼kle
        success = ingest_from_docs(UPLOAD_DIR)
        
        if not success:
            raise HTTPException(
                status_code=500, 
                detail="PDF iÅŸlenemedi"
            )

        # âœ… Global retriever'Ä± gÃ¼ncelle
        global retriever
        retriever = get_retriever()
        
        # âœ… Chain'leri yeniden baÅŸlat
        chain_success = initialize_chains()
        
        if not chain_success:
            raise HTTPException(
                status_code=500,
                detail="Chain'ler baÅŸlatÄ±lamadÄ±"
            )

        print("âœ… PDF yÃ¼klendi ve sistem gÃ¼ncellendi")

        return {
            "status": "success",
            "filename": file.filename,
            "size_bytes": len(content),
            "message": f"âœ… {file.filename} baÅŸarÄ±yla yÃ¼klendi ve iÅŸlendi"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"Hata: {str(e)}"
        )


# if __name__=='__main__':
#     session_id = "user123"
    
#     print("Welcome to the interactive Travel Assistant! Type 'quit' to exit.")
    
#     while True:
#         question = input("\nYour question: ")
        
#         if question.lower() in ['quit', 'exit', 'q']:
#             print("Goodbye!")
#             break
            
#         try:
#             response = qa_chain.invoke(
#                 {"input": question},
#                 config={"configurable": {"session_id": session_id}}
#             )
#             print(f"\nAnswer: {response['answer']}")
#         except Exception as e:
#             print(f"Error: {e}")